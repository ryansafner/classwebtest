---
title: "Class 7: Linear Regression"
citeproc: false
bibliography: ../../static/bib/references.bib
csl: ../../static/bib/chicago-syllabus-no-bib.csl
output:
  blogdown::html_page:
    template: ../../pandoc/toc-title_html.template
    toc: true
editor_options: 
  chunk_output_type: console
---

<!-- BLOGDOWN-HEAD -->
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>
<!-- /BLOGDOWN-HEAD -->

<h2>Contents</h2>
<div id="TOC">
<ul>
<li><a href="#covariance-and-correlation">Covariance and Correlation</a><ul>
<li><a href="#variance">Variance</a></li>
<li><a href="#covariance">Covariance</a></li>
<li><a href="#correlation">Correlation</a></li>
</ul></li>
<li><a href="#calculating-correlation-example">Calculating Correlation Example</a></li>
</ul>
</div>

<h2 id="covariance-and-correlation">Covariance and Correlation</h2>
<h3 id="variance">Variance</h3>
<p>Recall the variance of a random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(var(X)\)</span> or <span class="math inline">\(\sigma^2\)</span>, is the expected value (probability-weighted average) of the squared deviations of <span class="math inline">\(X_i\)</span> from it’s mean (or expected value) <span class="math inline">\(\bar{X}\)</span> or <span class="math inline">\(E(X)\)</span>.<span><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">Note there will be a different in notation depending on whether we refer to a population (e.g. <span class="math inline">\(\mu_{X}\)</span>) or to a sample (e.g. <span class="math inline">\(\bar{X}\)</span>). As the overwhelming majority of cases we will deal with samples, I will use sample notation for means).<br />
<br />
</span></span></p>
<p><span class="math display">\[\begin{align*}
\sigma_X^2&amp;=E(X-E(X))\\
&amp;=\sum^n_{i=1} (X_i-\bar{X})^2 p_i
\end{align*}\]</span></p>
<p>Note if all possible values of <span class="math inline">\(X_i\)</span> are equally likely (or we don’t know the probabilities), we can write variance as a simple average of squared deviations from the mean:</p>
<p><span class="math display">\[\begin{align*}
\sigma_X^2&amp;=\frac{1}{n}\sum^n_{i=1}(X_i-\bar{X})^2  
\end{align*}\]</span></p>
<p>Variance has some useful properties:</p>
<p><strong>Property 1</strong>: The variance of a constant is 0</p>
<p><span class="math display">\[var(c)=0 \text{ iff } P(X=c)=1\]</span></p>
<p>If a random variable takes the same value (e.g. 2) with probability 1.00, <span class="math inline">\(E(2)=2\)</span>, so the average squared deviation from the mean is 0, because there are never any values other than 2.</p>
<p><strong>Property 2</strong>: The variance is unchanged for a random variable plus/minus a constant</p>
<p><span class="math display">\[var(X\pm c)\]</span></p>
<p>Since the variance of a constant is 0.</p>
<p><strong>Property 3</strong>: The variance of a scaled random variable is scaled by the square of the coefficient</p>
<p><span class="math display">\[var(aX)=a^2var(X)\]</span></p>
<p><strong>Property 4</strong>: The variance of a linear transformation of a random variable is scaled by the square of the coefficient</p>
<p><span class="math display">\[var(aX+b)=a^2var(X)\]</span></p>
<h3 id="covariance">Covariance</h3>
<p>For two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we can measure their <strong>covariance</strong> (denoted <span class="math inline">\(cov(X,Y)\)</span> or <span class="math inline">\(\sigma_{X,Y}\)</span>)<span><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">Again, to be technically correct, <span class="math inline">\(\sigma_{X,Y}\)</span> refers to populations, <span class="math inline">\(s_{X,Y}\)</span> refers to samples, in line with population vs. sample variance and standard deviation. Recall also that sample estimates of variance and standard deviation divide by <span class="math inline">\(n-1\)</span>, rather than <span class="math inline">\(n\)</span>. In large sample sizes, this difference is negligible.<br />
<br />
</span></span> to quantify how they vary <em>together</em>. A good way to think about this is: when <span class="math inline">\(X\)</span> is above its mean, would we expect <span class="math inline">\(Y\)</span> to also be above its mean (and covary positively), or below its mean (and covary negatively). Remember, this is describing the <em>joint</em> probability distribution for two random variables.</p>
<p><span class="math display">\[\begin{align*}
\sigma_{X,Y}&amp;=E\big[(X-\bar{X})(Y-\bar{Y})\big]
\end{align*}\]</span></p>
<p>Again, in the case of equally probable values for both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, covariance is sometimes written:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{X,Y}&amp;=\frac{1}{N}\sum_{i=1}^n(X-\bar{X})(Y-\bar{Y})
\end{align*}\]</span></p>
<p>Covariance also has a number of useful properties:</p>
<p><strong>Property 1</strong>: The covariance of a random variable <span class="math inline">\(X\)</span> and a constant <span class="math inline">\(c\)</span> is 0</p>
<p><span class="math display">\[cov(X,c)=0\]</span></p>
<p><strong>Property 2</strong>: The covariance of a random variable and itself is the variable’s variance</p>
<p><span class="math display">\[\begin{align*}
    cov(X,X)&amp;=var(X)\\
    \sigma_{X,X}&amp;=\sigma^2_X\\
    \end{align*}\]</span></p>
<p><strong>Property 3</strong>: The covariance of a two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> each scaled by a constant <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is the product of the covariance and the constants</p>
<p><span class="math display">\[cov(aX,bY)=a\times b \times cov(X,Y)\]</span></p>
<p><strong>Property 4</strong>: If two random variables are independent, their covariance is 0</p>
<p><span class="math display">\[cov(X,Y)=0 \text{ iff } X \text{ and } Y \text{ are independent:}  E(XY)=E(X)\times E(Y)\]</span></p>
<h3 id="correlation">Correlation</h3>
<p>Covariance, like variance, is often cumbersome, and the numerical value of the covariance of two random variables does not really mean much. It is often convenient to normalize the covariance to a decimal between <span class="math inline">\(-1\)</span> and 1. We do this by dividing by the product of the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This is known as the <strong>correlation coefficient</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, denoted <span class="math inline">\(corr(X,Y)\)</span> or <span class="math inline">\(\rho_{X,Y}\)</span> (for populations) or <span class="math inline">\(r_{X,Y}\)</span> (for samples):</p>
<p><span class="math display">\[\begin{align*}    
r_{X,Y}&amp;=\frac{cov(X,Y)}{sd(X)sd(Y)}\\
&amp;=\frac{E\big[(X-\bar{X})(Y-\bar{Y})\big]}{\sqrt{E\big[X-\bar{X}\big]}\sqrt{E\big[Y-\bar{Y}\big]}}\\
&amp;=\frac{\sigma_{X,Y}}{\sigma_X \sigma_Y}\\
\end{align*}\]</span></p>
<p>Note this also means that covariance is the product of the standard deviation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and their correlation coefficient:</p>
<p><span class="math display">\[\begin{align*}
\sigma_{X,Y}&amp;=r_{X,Y}\sigma_X \sigma_Y  \\
cov(X,Y)&amp;=corr(X,Y)\times sd(X) \times sd(Y)    \\
\end{align*}\]</span></p>
<p>Another way to reach the (sample) correlation coefficient is by finding the average joint <span class="math inline">\(Z\)</span>-score of each pair of <span class="math inline">\((X_i,Y_i)\)</span>:</p>
<p><span class="math display">\[\begin{align*}    
r_{X,Y}&amp;=\frac{1}{n}\frac{\displaystyle\sum^n_{i=1}(X_i-\bar{X})(Y_i-\bar{Y}))}{s_X s_Y} &amp;&amp; \text{Definition of sample correlation}\\
&amp;=\frac{1}{n}\displaystyle\sum^n_{i=1}\bigg(\frac{X_i-\bar{X}}{s_X}\bigg)\bigg(\frac{Y_i-\bar{Y}}{s_Y}\bigg) &amp;&amp; \text{Breaking into separate sums} \\
&amp;=\frac{1}{n}\displaystyle\sum^n_{i=1}(Z_X)(Z_Y) &amp;&amp; \text{Recognize each sum is the z-score for that r.v.} \\
\end{align*}\]</span></p>
<p>Correlation has some useful properties that should be familiar to you:</p>
<ul>
<li>Correlation is between <span class="math inline">\(-1\)</span> and 1</li>
<li>A correlation of -1 is a downward sloping straight line</li>
<li>A correlation of 1 is an upward sloping straight line</li>
<li>A correlation of 0 implies no relationship</li>
</ul>
<h2 id="calculating-correlation-example">Calculating Correlation Example</h2>
<p>We can calculate the correlation of a simple data set (of 4 observations) using <code>R</code> to show how correlation is calculated. We will use the <span class="math inline">\(Z\)</span>-score method. Begin with a simple set of data in <span class="math inline">\((X_i, Y_i)\)</span> points:</p>
<p><span class="math display">\[ (1,1), (2,2), (3,4), (4,9) \]</span></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)

corr.example&lt;-<span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>),
                         <span class="dt">y=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">9</span>))
<span class="kw">ggplot</span>(corr.example,<span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>y))<span class="op">+</span><span class="kw">geom_point</span>()</code></pre>
<p><img src="/class/07-class_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(corr.example<span class="op">$</span>x) <span class="co">#find mean of x</span></code></pre>
<pre><code>## [1] 2.5</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(corr.example<span class="op">$</span>y) <span class="co">#find mean of y</span></code></pre>
<pre><code>## [1] 4</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(corr.example<span class="op">$</span>x) <span class="co">#find sd of x</span></code></pre>
<pre><code>## [1] 1.290994</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(corr.example<span class="op">$</span>y) <span class="co">#find sd of y</span></code></pre>
<pre><code>## [1] 3.559026</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#take z score of x,y for each pair and multiply them</span>
corr.example<span class="op">$</span>z.product&lt;-(((corr.example<span class="op">$</span>x<span class="fl">-2.5</span>)<span class="op">/</span><span class="fl">1.291</span>)<span class="op">*</span>
<span class="st">                           </span>((corr.example<span class="op">$</span>y<span class="dv">-4</span>)<span class="op">/</span><span class="fl">3.559</span>))

corr.example </code></pre>
<pre><code>##   x y z.product
## 1 1 1 0.9793959
## 2 2 2 0.2176435
## 3 3 4 0.0000000
## 4 4 9 1.6323265</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">sum</span>(corr.example<span class="op">$</span>z.product)<span class="op">/</span><span class="dv">3</span>) <span class="co">#average z products over n-1</span></code></pre>
<pre><code>## [1] 0.943122</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(corr.example<span class="op">$</span>x, corr.example<span class="op">$</span>y) <span class="co">#compare our answer to cor() command</span></code></pre>
<pre><code>## [1] 0.9431191</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cov</span>(corr.example<span class="op">$</span>x, corr.example<span class="op">$</span>y) <span class="co">#just for kicks - covariance </span></code></pre>
<pre><code>## [1] 4.333333</code></pre>
